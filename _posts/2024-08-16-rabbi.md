---
layout: post
title: "The Mismeasure of Man and Models"
author: "Hannah Chen"
homepage: ""
date: 2024-08-16
tldr: "Evaluating Allocational Harms in Large Language Models"
editor: ""
label: ["bias", "fairness"]
---

Our work considers <i>allocational harms</i> that arise when model predictions are used to distribute scarce resources or opportunities.

## Current Bias Metrics Do Not Reliably Reflect Allocation Disparities
Several methods have been proposed to audit large language models (LLMs) for bias when used in critical decision-making, such as resume screening for hiring. Yet, these methods focus on <i>predictions</i>, without considering how the predictions are used to make <i>decisions</i>. In many settings, making decisions involve prioritizing options due to limited resource constraints. We find that prediction-based evaluation methods, which measure bias as the <i>average performance gap</i> (δ) in prediction outcomes, do not reliably reflect disparities in allocation decision outcomes.

<center>
<img src="{{ site.url }}/figures/rabbi2024/llama2-7b-example.png" width="50%" align="center">
<div class="caption">Bias scores per group, computed with respect to White Male. δ: average performance gap, measured as the average score difference. ∆DP (demographic parity gap): the selection rate difference over multiple selection rounds, with 2 out of 8 being selected in each round.</div>
</center>


## Measuring Allocational Harms
We introduce <i>Rank-Allocational-Based Bias Index</i> (RABBI), a model-agnostic bias metric that measures allocational bias using scores derived from model outputs. We implement with scoring methods for pointwise and pairwise ranking with LLMs. Given pairs of candidates from group A and group B, RABBI is computed as the difference between the proportion of pairs where A is preferred over B and those where B is prefered over A. Our approach is inspired by the <i>rank-biserial correlation</i> [(Cureton, 1956)](https://link.springer.com/article/10.1007/BF02289138), which measures if the group membership is correlated with being higher-ranked or lower-ranked.

<center>
<img src="{{ site.url }}/figures/rabbi2024/rabbi_illustration.png" width="55%" align="center">
<div class="caption"><b>Rank-Allocational-Based Bias Index (RABBI)</b></div>
</center>
<br>

## Testing Predictive Validity
We compare bias scores measured with RABBI and traditional bias metrics to allocation gaps (∆DP and ∆EO) measured in simulated candidate selection outcomes. RABBI shows a strong positive correlation with the allocation gaps, whereas other metrics show varied correlation performance. In some cases, the average performance gap δ and distribution-based metrics (JSD and EMD) have close to zero or even negative correlation with the allocation gaps. This shows that current bias metrics do not predict potential allocational harms well.

<div class="row">
    <div class="column">{% include rabbi2024/pointwise_metric_corr.html %}</div>
    <div class="column">{% include rabbi2024/pointwise_dist_metric_corr.html %}</div>
</div>
<div class="caption">Measurement between bias metrics and allocation gaps on the resume screening task. Each point represents a score measured between White Male and each of the other groups for a job position. ∆DP: demographic parity gap, ∆EO: equal opportunity gap.</div>

## Metric Utility for Model Selection
We evaluate the utility of a metric for model selection by comparing the model fairness ranking derived from bias metrics to an ideal ranking. RABBI demonstrate the highest resemblance to ideal rankings based on the allocation gaps, as reported by the average normalized discounted cumulative gain (NDCG) at rank cutoff N.
<center>
<img src="{{ site.url }}/figures/rabbi2024/model_DP_rank_ndcg.png" width="60%" align="center">
<div class="caption">Average NDCG@N in ranking model fairness, comparing to ideal rankings based on ∆DP.</div>
</center>
<br>

We further compare the fairness ranking of models between different metrics for the resume screening task. RABBI's ranking aligns more closely with the ranking based on the allocation gap, whereas other metrics tend to rank more biased models higher. This demonstrates the effectiveness of RABBI in selecting models that diminish potential harm.

<div class="row">
    <div class="column" style="margin-top: 25px; margin-right: 10px;">{% include rabbi2024/model_fairness_rank.html %}</div>
    <div class="column" style="margin-left: 10px;">{% include rabbi2024/model_rank_per_job.html %}</div>
</div>
<div class="caption">Fairness ranking of models on the resume screening task. The true rank is based on ∆DP. (Left: overall ranking, Right: ranking per job position)</div>

## Conclusion
Our analysis reveal that commonly-used bias metrics based on average performance gap and distribution distance are insufficient to assess allocational harms. We propose an allocational bias measure, which consistently demonstrates better correlations with group disparities in allocation outcomes. Our results underscore the importance of considering how models will be used in deployment to develop reliable auditing methods.


<b>Paper</b>: [Hannah Chen](https://hannahxchen.github.io/), [Yangfeng Ji](https://yangfengji.net/), [David Evans](http://www.cs.virginia.edu/~evans/). [The Mismeasure of Man and Models: Evaluating Allocational Harms in Large Language Models](https://arxiv.org/abs/2408.01285). In arXiv Preprint, 02 August 2024.

<b>Code</b>: [https://github.com/hannahxchen/llm-allocational-harm-eval](https://github.com/hannahxchen/llm-allocational-harm-eval)
